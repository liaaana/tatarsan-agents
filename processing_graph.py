from __future__ import annotations

import json
from dataclasses import dataclass, asdict
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, TypedDict

from dotenv import load_dotenv
from langgraph.graph import StateGraph, END

from pydantic import BaseModel, Field
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_openai import ChatOpenAI


# ---------- 0. –ó–∞–≥—Ä—É–∑–∫–∞ –¢–£ ----------

TU_DIR = Path(__file__).with_name("tu")


def load_all_tu_configs() -> Dict[str, Dict[str, Any]]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ *.json –∏–∑ –ø–∞–ø–∫–∏ tu/ –∏ —Å—Ç—Ä–æ–∏–º —Å–ª–æ–≤–∞—Ä—å:
    { tu_id: {"meta": {...}, "data": {...}} }
    –≥–¥–µ tu_id = –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è "id" –≤–Ω—É—Ç—Ä–∏ JSON
    """
    configs: Dict[str, Dict[str, Any]] = {}

    if not TU_DIR.exists():
        print(f"[WARN] –ü–∞–ø–∫–∞ —Å –¢–£ {TU_DIR} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
        return configs

    for p in TU_DIR.glob("*.json"):
        try:
            with p.open("r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            print(f"[WARN] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¢–£ {p}: {e}")
            continue

        tu_id = str(data.get("id") or p.stem)
        meta_name = f"–¢–£ {tu_id}"

        configs[tu_id] = {
            "meta": {
                "id": tu_id,
                "name": meta_name,
                "file": str(p),
            },
            "data": data,
        }

    return configs


ALL_TU_CONFIGS = load_all_tu_configs()

DEFAULT_TU_ID = os.getenv("TU_ID")
if DEFAULT_TU_ID not in ALL_TU_CONFIGS and ALL_TU_CONFIGS:
    DEFAULT_TU_ID = next(iter(ALL_TU_CONFIGS))  # –ø–µ—Ä–≤—ã–π –ø–æ–ø–∞–≤—à–∏–π—Å—è


# ---------- 1. –û–±—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ ----------


class AppState(TypedDict, total=False):
    # –í—Ö–æ–¥
    file_path: str
    file_ext: str
    file_bytes: bytes

    # –í—ã–±—Ä–∞–Ω–Ω–æ–µ –¢–£
    tu_id: str  # –Ω–∞–ø—Ä–∏–º–µ—Ä "3667-013-05608841-2020"

    # –¢–µ–∫—Å—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
    raw_text: str

    # –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞—è–≤–∫–∞
    request_fields: Dict[str, Any]      # –∫–∞–∫ –≤–µ—Ä–Ω—É–ª LLM
    matched_items: List[Dict[str, Any]] # –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ
    export_payload: Dict[str, Any]

    # –õ–æ–≥ —à–∞–≥–æ–≤ –¥–ª—è UI
    messages: List[str]


# ---------- 2. –ú–æ–¥–µ–ª—å –∑–∞—è–≤–∫–∏ (–ù–≠–ú–°) ----------


class RequestFieldsModel(BaseModel):
    """–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ù–≠–ú–° –∏–∑ –¢–£, –≤ —Ç–µ—Ä–º–∏–Ω–∞—Ö –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è."""

    dn_mm: Optional[int] = Field(
        None,
        description="–ù–∞—Ä—É–∂–Ω—ã–π –¥–∏–∞–º–µ—Ç—Ä –ø–∞—Ç—Ä—É–±–∫–æ–≤ –î–Ω, –º–º (–≤ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏–∏: –≤—Ç–æ—Ä–æ–µ –ø–æ–ª–µ, –Ω–∞–ø—Ä–∏–º–µ—Ä 325)",
    )
    pressure_kgf_cm2: Optional[float] = Field(
        None,
        description="–†–∞–±–æ—á–µ–µ –¥–∞–≤–ª–µ–Ω–∏–µ, –∫–≥—Å/—Å–º¬≤ (—Ç—Ä–µ—Ç—å–µ –ø–æ–ª–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä 40)",
    )
    length_mm: Optional[int] = Field(
        None,
        description="–î–ª–∏–Ω–∞ –∏–∑–¥–µ–ª–∏—è, –º–º (—á–µ—Ç–≤—ë—Ä—Ç–æ–µ –ø–æ–ª–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä 800)",
    )
    medium_code: Optional[str] = Field(
        None,
        description="–ö–æ–¥ —Å—Ä–µ–¥—ã, –Ω–∞–ø—Ä–∏–º–µ—Ä '–í–î' ‚Äî —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –∏–ª–∏ –ø–∏—Ç—å–µ–≤–∞—è –≤–æ–¥–∞ (–ø—è—Ç–æ–µ –ø–æ–ª–µ –æ–±–æ–∑–Ω–∞—á–µ–Ω–∏—è)",
    )
    placement_code: Optional[str] = Field(
        None,
        description="–ö–æ–¥ –º–µ—Å—Ç–∞ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –Ω–∞ —Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥–µ (–ø–µ—Ä–≤–∞—è —Ü–∏—Ñ—Ä–∞ –≤ –≥—Ä—É–ø–ø–µ '1-2' –∏ —Ç.–ø.)",
    )
    connection_code: Optional[str] = Field(
        None,
        description="–ö–æ–¥ —Ç–∏–ø–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å —Ç—Ä—É–±–æ–ø—Ä–æ–≤–æ–¥–æ–º (–≤—Ç–æ—Ä–∞—è —Ü–∏—Ñ—Ä–∞ –≤ –≥—Ä—É–ø–ø–µ '1-2', –Ω–∞–ø—Ä–∏–º–µ—Ä —Å–≤–∞—Ä–∫–∞ —Å –Ω–∞–∫–æ–Ω–µ—á–Ω–∏–∫–æ–º)",
    )
    inner_coating_code: Optional[str] = Field(
        None,
        description="–ö–æ–¥ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –∑–∞—â–∏—Ç–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è (–ø–µ—Ä–≤–∞—è —Ü–∏—Ñ—Ä–∞ –≤ –≥—Ä—É–ø–ø–µ '4-3')",
    )
    outer_coating_code: Optional[str] = Field(
        None,
        description="–ö–æ–¥ –Ω–∞—Ä—É–∂–Ω–æ–≥–æ –∑–∞—â–∏—Ç–Ω–æ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è (–≤—Ç–æ—Ä–∞—è —Ü–∏—Ñ—Ä–∞ –≤ –≥—Ä—É–ø–ø–µ '4-3')",
    )
    terminals_code: Optional[str] = Field(
        None,
        description="–ü—Ä–∏–∑–Ω–∞–∫ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∫–ª–µ–º–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–ö' ‚Äî –∫–ª–µ–º–º—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã, –ø—É—Å—Ç–æ ‚Äî –±–µ–∑ –∫–ª–µ–º–º)",
    )
    climate_code: Optional[str] = Field(
        None,
        description="–ö–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–æ –ì–û–°–¢ 15150 (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–£1', '–£–î')",
    )
    notes: Optional[str] = Field(
        None,
        description="–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è / –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏–∑ –∑–∞—è–≤–∫–∏",
    )


@dataclass
class RequestFields:
    dn_mm: Optional[int] = None
    pressure_kgf_cm2: Optional[float] = None
    length_mm: Optional[int] = None
    medium_code: Optional[str] = None
    placement_code: Optional[str] = None
    connection_code: Optional[str] = None
    inner_coating_code: Optional[str] = None
    outer_coating_code: Optional[str] = None
    terminals_code: Optional[str] = None
    climate_code: Optional[str] = None
    notes: Optional[str] = None


# ---------- 3. LLM (LangChain) ----------

load_dotenv()
api_key = os.getenv("OPENAI_API_KEY") or os.getenv("GPT_API_KEY")
if not api_key:
    raise RuntimeError(
        "–ù–µ –Ω–∞–π–¥–µ–Ω API-–∫–ª—é—á. –£—Å—Ç–∞–Ω–æ–≤–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY "
        "–∏–ª–∏ GPT_API_KEY –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º."
    )

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.1,
    api_key=api_key,
)

structured_llm = llm.with_structured_output(RequestFieldsModel)


# ---------- 4. –£—Ç–∏–ª–∏—Ç—ã (OCR/–ø–∞—Ä—Å–∏–Ω–≥) ----------


def add_msg(state: AppState, text: str) -> None:
    msgs = state.get("messages") or []
    msgs.append(text)
    state["messages"] = msgs


def read_file_bytes(path: str) -> bytes:
    with open(path, "rb") as f:
        return f.read()


def extract_text_from_file(path: str, ext: str, data: bytes) -> str:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ñ–∞–π–ª–∞:

    - PNG/JPEG/PDF ‚Üí –Ø–Ω–¥–µ–∫—Å OCR
    - DOCX ‚Üí python-docx (–ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã + —Ç–∞–±–ª–∏—Ü—ã)
    - XLS/XLSX ‚Üí pandas (—Å–∫–ª–µ–∏–≤–∞–µ–º –≤—Å–µ –ª–∏—Å—Ç—ã)
    - –æ—Å—Ç–∞–ª—å–Ω–æ–µ ‚Üí –ø—ã—Ç–∞–µ–º—Å—è –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–∫ —Ç–µ–∫—Å—Ç
    """
    from docx import Document
    import pandas as pd
    from yandex_ocr_client import recognize_file_to_text, YandexOcrError
    
    p = Path(path)
    ext = ext.lower()

    try:
        # 1) –ö–∞—Ä—Ç–∏–Ω–∫–∏ –∏ PDF ‚Äî —á–µ—Ä–µ–∑ –Ø–Ω–¥–µ–∫—Å OCR
        if ext in [".png", ".jpg", ".jpeg", ".pdf"]:
            return recognize_file_to_text(str(p))

        # 2) DOCX ‚Äî —á–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —Ç–∞–±–ª–∏—Ü—ã
        if ext == ".docx":
            doc = Document(path)
            parts: List[str] = []

            for para in doc.paragraphs:
                t = para.text.strip()
                if t:
                    parts.append(t)

            for table in doc.tables:
                for row in table.rows:
                    cells = [cell.text.strip() for cell in row.cells]
                    row_text = "\t".join(c for c in cells if c)
                    if row_text:
                        parts.append(row_text)

            return "\n".join(parts)

        # 3) Excel ‚Äî —á–∏—Ç–∞–µ–º –≤—Å–µ –ª–∏—Å—Ç—ã –∏ —Å–∫–ª–µ–∏–≤–∞–µ–º
        if ext in [".xls", ".xlsx"]:
            sheets = pd.read_excel(path, sheet_name=None)
            blocks: List[str] = []
            for sheet_name, df in sheets.items():
                blocks.append(f"### {sheet_name}")
                blocks.append(df.to_string(index=False))
            return "\n\n".join(blocks)

        # 4) –°—Ç–∞—Ä—ã–π .doc ‚Äî –ª—É—á—à–µ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ç—å –≤ PDF/Docx –¥–æ –∑–∞–≥—Ä—É–∑–∫–∏
        if ext == ".doc":
            raise YandexOcrError(
                ".doc —Å–µ–π—á–∞—Å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è. "
                "–ü–æ–ø—Ä–æ—Å–∏ –∫–ª–∏–µ–Ω—Ç–∞ –ø—Ä–∏—Å–ª–∞—Ç—å PDF –∏–ª–∏ DOCX."
            )

        # 5) –§–æ–ª–±—ç–∫ ‚Äî –ø—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–Ω—É—Ç—å –±–∞–π—Ç—ã –∫–∞–∫ —Ç–µ–∫—Å—Ç
        return data.decode("utf-8", errors="ignore")

    except Exception as e:
        print(f"[extract_text_from_file error] {e}")
        return ""


def match_with_catalog(fields: RequestFields) -> List[Dict[str, Any]]:
    """
    –ó–∞–≥–ª—É—à–∫–∞ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –ù–≠–ú–°.
    –°–µ–π—á–∞—Å –ø—Ä–æ—Å—Ç–æ —Å–æ–±–∏—Ä–∞–µ–º —á–∏—Ç–∞–µ–º–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –ø–æ–ª–µ–π –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–¥–∏–Ω –≤–∞—Ä–∏–∞–Ω—Ç.
    """
    desc_parts: List[str] = []

    if fields.dn_mm is not None:
        desc_parts.append(f"–î–Ω {fields.dn_mm} –º–º")
    if fields.pressure_kgf_cm2 is not None:
        desc_parts.append(f"PN {fields.pressure_kgf_cm2} –∫–≥—Å/—Å–º¬≤")
    if fields.length_mm is not None:
        desc_parts.append(f"L={fields.length_mm} –º–º")
    if fields.medium_code:
        desc_parts.append(f"—Å—Ä–µ–¥–∞ {fields.medium_code}")
    if fields.placement_code:
        desc_parts.append(f"—Ä–∞–∑–º–µ—â–µ–Ω–∏–µ {fields.placement_code}")
    if fields.connection_code:
        desc_parts.append(f"—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ {fields.connection_code}")
    if fields.inner_coating_code:
        desc_parts.append(f"–≤–Ω—É—Ç—Ä. –ø–æ–∫—Ä—ã—Ç–∏–µ {fields.inner_coating_code}")
    if fields.outer_coating_code:
        desc_parts.append(f"–Ω–∞—Ä—É–∂. –ø–æ–∫—Ä—ã—Ç–∏–µ {fields.outer_coating_code}")
    if fields.terminals_code:
        desc_parts.append(f"–∫–ª–µ–º–º—ã {fields.terminals_code}")
    if fields.climate_code:
        desc_parts.append(f"–∫–ª–∏–º–∞—Ç {fields.climate_code}")

    name = "–ù–≠–ú–°"
    if desc_parts:
        name += " (" + ", ".join(desc_parts) + ")"

    return [
        {
            "item_code": "NEMS-PLACEHOLDER",
            "name": name,
            "score": 0.8,
            "matched_fields": asdict(fields),
        }
    ]


def build_export_payload(fields: RequestFields, matched: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    –°—Ç—Ä—É–∫—Ç—É—Ä–∞, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ 1–° / –∑–∞–ø–∏—Å–∞—Ç—å –≤ Excel / CSV.
    """
    return {
        "request_fields": asdict(fields),
        "matched_items": matched,
    }


# ---------- 5. –£–∑–ª—ã (–∞–≥–µ–Ω—Ç—ã) LangGraph ----------


def file_ingestion_node(state: AppState) -> AppState:
    path = state.get("file_path")
    if not path:
        raise ValueError("file_path is missing in state")

    ext = Path(path).suffix.lower()
    data = read_file_bytes(path)

    state["file_ext"] = ext
    state["file_bytes"] = data
    add_msg(state, f"[file_ingestion] Loaded file {Path(path).name} (ext={ext}, size={len(data)} bytes).")
    return state


def text_extraction_node(state: AppState) -> AppState:
    path = state.get("file_path")
    ext = state.get("file_ext")
    data = state.get("file_bytes")

    if not path or ext is None or data is None:
        raise ValueError("file_path/file_ext/file_bytes not set for text extraction")

    text = extract_text_from_file(path, ext, data)
    state["raw_text"] = text
    add_msg(state, f"[text_extraction] Extracted text of length {len(text)} chars.")
    # üëá –ª–æ–≥–∏—Ä—É–µ–º –ø—Ä–µ–≤—å—é —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (OCR / –ø–∞—Ä—Å–∏–Ω–≥)
    preview = text[:500].replace("\n", " ")
    add_msg(state, f"[text_extraction][preview] {preview}")
    return state


def field_extraction_node(state: AppState) -> AppState:
    text = state.get("raw_text", "")

    # –ë–µ—Ä—ë–º –¢–£: –ª–∏–±–æ –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è (state["tu_id"]), –ª–∏–±–æ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
    tu_id = state.get("tu_id") or DEFAULT_TU_ID
    tu_cfg = ALL_TU_CONFIGS.get(tu_id)

    if not tu_cfg:
        add_msg(state, f"[field_extraction] TU config '{tu_id}' –Ω–µ –Ω–∞–π–¥–µ–Ω, —Ä–∞–±–æ—Ç–∞—é –±–µ–∑ –¢–£.")
        tu_json_for_prompt = "{}"
    else:
        tu_json_for_prompt = json.dumps(tu_cfg["data"], ensure_ascii=False, indent=2)

    system_msg = SystemMessage(
        content=(
            "–¢—ã –∏–∑–≤–ª–µ–∫–∞–µ—à—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ù–≠–ú–° (–Ω–µ—Ä–∞–∑—ä–µ–º–Ω–æ–µ —ç–ª–µ–∫—Ç—Ä–æ–∏–∑–æ–ª–∏—Ä—É—é—â–µ–µ –º—É—Ñ—Ç–æ–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ) "
            "–∏–∑ –æ–ø—Ä–æ—Å–Ω–æ–≥–æ –ª–∏—Å—Ç–∞/–∑–∞—è–≤–∫–∏ –∏ –ø—Ä–∏–≤–æ–¥–∏—à—å –∏—Ö –∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ RequestFieldsModel.\n\n"
            f"–ò—Å–ø–æ–ª—å–∑—É–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ —Å–ª–µ–¥—É—é—â–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —É—Å–ª–æ–≤–∏—è (–¢–£ {tu_id}) –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:\n"
            f"{tu_json_for_prompt}\n\n"
            "–ü—Ä–∞–≤–∏–ª–∞:\n"
            "1. –ù–∏—á–µ–≥–æ –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π ‚Äî –µ—Å–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä –Ω–µ —É–∫–∞–∑–∞–Ω –∏ –Ω–µ –≤—ã–≤–æ–¥–∏—Ç—Å—è –æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ –∏–∑ –¢–£, –æ—Å—Ç–∞–≤–ª—è–π null.\n"
            "2. –ï—Å–ª–∏ –¥–∞–≤–ª–µ–Ω–∏–µ —É–∫–∞–∑–∞–Ω–æ –≤ –ú–ü–∞, –º–æ–∂–µ—à—å –ø–æ–¥–æ–±—Ä–∞—Ç—å –±–ª–∏–∂–∞–π—à–∏–π –∫–ª–∞—Å—Å –∏–∑ pressure_classes.\n"
            "3. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω–∞ —Å—Ä–µ–¥–∞, —Å–æ–ø–æ—Å—Ç–∞–≤—å –µ—ë —Å –∫–æ–¥–æ–º –∏–∑ product_types (–ú–ì, –†–°, –ù–ü, –í–î, –¢–° –∏ —Ç.–ø.).\n"
            "4. –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã —è–≤–Ω—ã–µ –∫–æ–¥—ã (–í–î, –£1, —Ü–∏—Ñ—Ä—ã –ø–æ–∫—Ä—ã—Ç–∏–π –∏ –¥—Ä.), –∏—Å–ø–æ–ª—å–∑—É–π –∏—Ö –∫–∞–∫ –µ—Å—Ç—å, —Å–≤–µ—Ä—è—è—Å—å —Å JSON –¢–£.\n"
        )
    )

    user_msg = HumanMessage(
        content=(
            "–í–æ—Ç —Ç–µ–∫—Å—Ç –æ–ø—Ä–æ—Å–Ω–æ–≥–æ –ª–∏—Å—Ç–∞/–∑–∞—è–≤–∫–∏. "
            "–ó–∞–ø–æ–ª–Ω–∏ —Å—Ö–µ–º—É RequestFieldsModel, –∏—Å–ø–æ–ª—å–∑—É—è JSON —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏ –≤—ã—à–µ.\n\n"
            + text[:6000]
        )
    )

    result: RequestFieldsModel = structured_llm.invoke([system_msg, user_msg])
    fields = RequestFields(**result.dict())
    state["request_fields"] = asdict(fields)
    add_msg(
        state,
        "[field_extraction] Extracted request fields: "
        + json.dumps(asdict(fields), ensure_ascii=False),
    )
    return state


def matching_node(state: AppState) -> AppState:
    if "request_fields" not in state:
        add_msg(state, "[matching] No request_fields in state, nothing to match.")
        return state

    fields = RequestFields(**state["request_fields"])
    items = match_with_catalog(fields)
    state["matched_items"] = items
    add_msg(state, "[matching] Found catalog matches: " + json.dumps(items, ensure_ascii=False))
    return state


def export_node(state: AppState) -> AppState:
    fields_dict = state.get("request_fields", {})
    items = state.get("matched_items", [])

    fields = RequestFields(**fields_dict) if fields_dict else RequestFields()
    payload = build_export_payload(fields, items)
    state["export_payload"] = payload
    add_msg(state, "[export] Built export payload.")
    return state


# ---------- 6. –°–±–æ—Ä–∫–∞ –≥—Ä–∞—Ñ–∞ ----------


def build_processing_graph():
    workflow = StateGraph(AppState)

    workflow.add_node("file_ingestion", file_ingestion_node)
    workflow.add_node("text_extraction", text_extraction_node)
    workflow.add_node("field_extraction", field_extraction_node)
    workflow.add_node("matching", matching_node)
    workflow.add_node("export", export_node)

    workflow.set_entry_point("file_ingestion")

    workflow.add_edge("file_ingestion", "text_extraction")
    workflow.add_edge("text_extraction", "field_extraction")
    workflow.add_edge("field_extraction", "matching")
    workflow.add_edge("matching", "export")
    workflow.add_edge("export", END)

    return workflow.compile()


# ---------- 7. –õ–æ–∫–∞–ª—å–Ω—ã–π —Ç–µ—Å—Ç ----------

if __name__ == "__main__":
    graph = build_processing_graph()
    example_path = "uploads/example.png"  # –ø–æ–¥—Å—Ç–∞–≤—å —Å–≤–æ–π –ø—É—Ç—å

    init_state: AppState = {
        "file_path": example_path,
        "messages": [],
        # "tu_id": "3667-013-05608841-2020",  # –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —è–≤–Ω–æ
    }

    final_state = graph.invoke(init_state)
    final_state.pop("file_bytes", None)

    print("=== FINAL STATE ===")
    print(json.dumps(final_state, ensure_ascii=False, indent=2))

    print("\n=== LOG ===")
    for m in final_state.get("messages", []):
        print(m)
